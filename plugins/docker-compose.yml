version: "3.9"
services:
  promptpanel:
    image: docker.io/promptpanel/promptpanel:latest
    container_name: promptpanel
    restart: always
    ports:
      - 4000:4000
    volumes:
      # Your new plugins
      # - ./new_plugin:/app/plugins/completion_chat
      # Community plugins
      - ./completion_chat:/app/plugins/completion_chat
      - ./completion_document:/app/plugins/completion_document
      - ./ollama_chat:/app/plugins/ollama_chat
      - ./ollama_document:/app/plugins/ollama_document
      - ./llm_chat:/app/plugins/llm_chat
      - ./llm_document:/app/plugins/llm_document
    environment:
      # PROMPT_MODE: DEVELOPMENT
      # PROMPT_DEV_REQS: DISABLED
      PROMPT_OLLAMA_HOST: http://ollama:11434
  ## Ollama is optional for local inference.
  ## You can remove this service & the `PROMPT_OLLAMA_HOST` environment variable in order to disable local inference.
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    ports:
      - 11434:11434
