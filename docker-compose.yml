version: "3.9"
services:
  promptpanel:
    image: promptpanel/promptpanel:latest
    container_name: promptpanel
    restart: always
    ports:
      - 4000:4000
    environment:
      PROMPT_OLLAMA_HOST: http://ollama:11434
  ## Ollama is optional for local inference.
  ## You can remove this service & the `PROMPT_OLLAMA_HOST` environment variable in order to disable local inference.
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    ports:
      - 11434:11434
